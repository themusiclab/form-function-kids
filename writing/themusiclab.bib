@Article{LiSubmitted,
  Author = {Li, Wei and Laura T. Germine and Samuel A. Mehr and Mahesh Srinivasan and Joshua K. Hartshorne},
  Title = {Building a generalizable and replicable developmental psychology with citizen science},
  Year = {submitted}
}

@Article{Bainbridge2021,
  Author = {Bainbridge, Constance M. and Bertolo, Mila and Youngers, Julie and Atwood, S. and Yurdum, Lidya and Simson, Jan and Lopez, Kelsie and Xing, Feng and Martin, Alia and Mehr, Samuel A.},
  Journal = {Nature Human Behaviour},
  Title = {Infants Relax in Response to Unfamiliar Foreign Lullabies},
  Year = {2021},
  Issn = {2397-3374},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/XYYUYA4A/Bainbridge et al. - 2020 - Infants relax in response to unfamiliar foreign lu.pdf},
  Doi = {10.1038/s41562-020-00963-z}
}


@Article{Balch1996,
  Author = {Balch, William R and Lewis, Benjamin S},
  Journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  Pages = {10},
  Title = {Music-{{Dependent Memory}}: {{The Roles}} of {{Tempo Change}} and {{Mood Mediation}}},
  Year = {1996},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/AQC7HKVN/Balch and Lewis - Music-Dependent Memory The Roles of Tempo Change .pdf}
}


@Article{Baldwin1996,
  Author = {Baldwin, Dare A. and Markman, Ellen M. and Bill, Brigitte and Desjardins, Renee N. and Irwin, Jane M. and Tidball, Glynnis},
  Journal = {Child Development},
  Pages = {20},
  Title = {Infants' {{Reliance}} on a {{Social Criterion}} for {{Establishing Word-Object Relations}}},
  Year = {1996},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/BJBYSJ95/2021 - Infants' Reliance on a Social Criterion for Establ.pdf}
}


@Article{Balkwill1999,
  Author = {Balkwill, Laura-Lee and Thompson, William Forde},
  Journal = {Music Perception},
  Month = {},
  Number = {1},
  Pages = {43--64},
  Title = {A Cross-Cultural Investigation of the Perception of Emotion in Music: {{Psychophysical}} and Cultural Cues},
  Volume = {17},
  Year = {1999},
  Issn = {0730-7829, 1533-8312},
  Langid = {english},
  Shorttitle = {A {{Cross-Cultural Investigation}} of the {{Perception}} of {{Emotion}} in {{Music}}},
  Doi = {10.2307/40285811}
}


@Article{Bergeson1999,
  Author = {Bergeson, Tonya and Trehub, Sandra},
  Journal = {Infant Behavior and Development},
  Number = {1},
  Title = {Mothers' Singing to Infants and Preschool Children},
  Volume = {22},
  Year = {1999},
  Keywords = {music [infants]}
}


@Article{Blasi2016,
  Author = {Blasi, Dami{\'a}n E. and Wichmann, S{\o}ren and Hammarstr{\"o}m, Harald and Stadler, Peter F. and Christiansen, Morten H.},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {39},
  Pages = {10818--10823},
  Title = {Sound\textendash Meaning Association Biases Evidenced across Thousands of Languages},
  Volume = {113},
  Year = {2016},
  Issn = {0027-8424, 1091-6490},
  Langid = {english},
  Doi = {10.1073/pnas.1605782113}
}


@Article{Bohn2019,
  Author = {Bohn, Manuel and Frank, Michael C},
  Journal = {Annual Review of Developmental Psychology},
  Pages = {29},
  Title = {The {{Pervasive Role}} of {{Pragmatics}} in {{Early Language}}},
  Year = {2019},
  Langid = {english},
  Abstract = {Language is a fundamentally social endeavor. Pragmatics is the study of how speakers and listeners use social reasoning to go beyond the literal meanings of words to interpret language in context. In this article, we take a pragmatic perspective on language development and argue for developmental continuity between early nonverbal communication, language learning, and linguistic pragmatics. We link phenomena from these different literatures by relating them to a computational framework (the rational speech act framework), which conceptualizes communication as fundamentally inferential and grounded in social cognition. The model specifies how different information sources (linguistic utterances, social cues, common ground) are combined when making pragmatic inferences. We present evidence in favor of this inferential view and review how pragmatic reasoning supports children's learning, comprehension, and use of language.},
  File = {/Users/courtneyhilton/Zotero/storage/4Z89G5JE/Bohn and Frank - 2019 - The Pervasive Role of Pragmatics in Early Language.pdf}
}


@Article{Bonneville-Roussy2013,
  Author = {{Bonneville-Roussy}, Arielle and Rentfrow, Peter J. and Xu, Man K. and Potter, Jeff},
  Journal = {Journal of Personality and Social Psychology},
  Month = {},
  Number = {4},
  Pages = {703--717},
  Title = {Music through the Ages: {{Trends}} in Musical Engagement and Preferences from Adolescence through Middle Adulthood.},
  Volume = {105},
  Year = {2013},
  Issn = {1939-1315, 0022-3514},
  Langid = {english},
  Abstract = {Are there developmental trends in how individuals experience and engage with music? Data from 2 large cross-sectional studies involving more than a quarter of a million individuals were used to investigate age differences in musical attitudes and preferences from adolescence through middle age. Study 1 investigated age trends in musical engagement. Results indicated that (a) the degree of importance attributed to music declines with age but that adults still consider music important, (b) young people listen to music significantly more often than do middle-aged adults, and (c) young people listen to music in a wide variety of contexts, whereas adults listen to music primarily in private contexts. Study 2 examined age trends in musical preferences. Results indicated that (a) musical preferences can be conceptualized in terms of a 5-dimensional age-invariant model, (b) certain music-preference dimensions decrease with age (e.g., Intense, Contemporary), whereas preferences for other music dimensions increase with age (e.g., Unpretentious, Sophisticated), and (c) age trends in musical preferences are closely associated with personality. Normative age trends in musical preferences corresponded with developmental changes in psychosocial development, personality, and auditory perception. Overall, the findings suggest that musical preferences are subject to a variety of developmental influences throughout the life span.},
  Shorttitle = {Music through the Ages},
  File = {/Users/courtneyhilton/Zotero/storage/DDL4MRKI/Bonneville-Roussy et al. - 2013 - Music through the ages Trends in musical engageme.pdf},
  Doi = {10.1037/a0033770}
}


@Article{Brandt2012,
  Author = {Brandt, Anthony and Gebrian, Molly and Slevc, L. Robert},
  Journal = {Frontiers in Psychology},
  Title = {Music and {{Early Language Acquisition}}},
  Volume = {3},
  Year = {2012},
  Issn = {1664-1078},
  Doi = {10.3389/fpsyg.2012.00327}
}


@Article{Cheever2018,
  Author = {Cheever, Thomas and Taylor, Anna and Finkelstein, Robert and Edwards, Emmeline and Thomas, Laura and Bradt, Joke and Holochwost, Steven J. and Johnson, Julene K. and Limb, Charles and Patel, Aniruddh D. and Tottenham, Nim and Iyengar, Sunil and Rutter, Deborah and Fleming, Ren{\'e}e and Collins, Francis S.},
  Journal = {Neuron},
  Number = {6},
  Pages = {1214--1218},
  Title = {{{NIH}}/{{Kennedy Center}} Workshop on Music and the Brain: {{Finding}} Harmony},
  Volume = {97},
  Year = {2018},
  Issn = {0896-6273},
  Langid = {english},
  Shorttitle = {{{NIH}}/{{Kennedy Center Workshop}} on {{Music}} and the {{Brain}}},
  Doi = {10.1016/j.neuron.2018.02.004},
  Pmid = {29566791}
}


@Article{Chronaki2018,
  Author = {Chronaki, Georgia and Wigelsworth, Michael and Pell, Marc D. and Kotz, Sonja A.},
  Journal = {Scientific Reports},
  Month = {},
  Number = {1},
  Pages = {8659},
  Title = {The Development of Cross-Cultural Recognition of Vocal Emotion during Childhood and Adolescence},
  Volume = {8},
  Year = {2018},
  Issn = {2045-2322},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/F34BXDRR/Chronaki et al. - 2018 - The development of cross-cultural recognition of v.pdf},
  Doi = {10.1038/s41598-018-26889-1}
}


@Article{Cirelli2019,
  Author = {Cirelli, Laura K. and Jurewicz, Zuzanna B. and Trehub, Sandra E.},
  Journal = {Journal of Cognitive Neuroscience},
  Title = {Effects of Maternal Singing Style on Mother\textendash Infant Arousal and Behavior},
  Year = {2019},
  Issn = {0898-929X, 1530-8898},
  Keywords = {to read},
  Langid = {english},
  Abstract = {Mothers around the world sing to infants, presumably to regulate their mood and arousal. Lullabies and playsongs differ stylistically and have distinctive goals. Mothers sing lullabies to soothe and calm infants and playsongs to engage and excite infants. In this study, mothers repeatedly sang Twinkle, Twinkle, Little Star to their infants ( n = 30 dyads), alternating between soothing and playful renditions. Infant attention and mother\textendash infant arousal (i.e., skin conductivity) were recorded continuously. During soothing renditions, mother and infant arousal decreased below initial levels as the singing progressed. During playful renditions, maternal and infant arousal remained stable. Moreover, infants exhibited greater attention to mother during playful renditions than during soothing renditions. Mothers' playful renditions were faster, higher in pitch, louder, and characterized by greater pulse clarity than their soothing renditions. Mothers also produced more energetic rhythmic movements during their playful renditions. These findings highlight the contrastive nature and consequences of lullabies and playsongs.},
  File = {/Users/courtneyhilton/Zotero/storage/5KJSKHMS/Cirelli et al. - 2020 - Effects of Maternal Singing Style on Motherâ€“Infant.pdf;/Users/courtneyhilton/Zotero/storage/RF8PTM5M/Effects-of-Maternal-Singing-Style-on-Mother-Infant.html},
  Doi = {10.1162/jocn_a_01402}
}


@Article{Cowen2019,
  Author = {Cowen, Alan S. and Laukka, Petri and Elfenbein, Hillary Anger and Liu, Runjing and Keltner, Dacher},
  Journal = {Nature Human Behaviour},
  Month = {},
  Number = {4},
  Pages = {369--382},
  Title = {The Primacy of Categories in the Recognition of 12 Emotions in Speech Prosody across Two Cultures},
  Volume = {3},
  Year = {2019},
  Issn = {2397-3374},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/ZGGEVEVM/Cowen et al. - 2019 - The primacy of categories in the recognition of 12.pdf},
  Doi = {10.1038/s41562-019-0533-6}
}


@Book{Darwin1871,
  Address = {{London}},
  Author = {Darwin, Charles},
  Publisher = {{Watts \& Co.}},
  Series = {The Thinker's Library},
  Title = {The Descent of Man},
  Year = {1871},
  Keywords = {added,evo},
  Lccn = {Museum Comp Zoology Vertebrate Paleontology Collection QH365 .D2 1930}
}


@Article{Drake2003,
  Author = {Drake, Carolyn and Heni, Jamel Ben El},
  Journal = {Annals of the New York Academy of Sciences},
  Number = {1},
  Pages = {429--437},
  Title = {Synchronizing with {{Music}}: {{Intercultural Differences}}},
  Volume = {999},
  Year = {2003},
  Issn = {1749-6632},
  Keywords = {hierarchical structure,intercultural studies,music synchronization,musical training,temporal organization,tim-r},
  Langid = {english},
  Abstract = {Abstract: The way in which listeners perceive music changes throughout childhood, but little is known about the factors responsible for these changes. One factor, explicit music training, has received considerable attention, with studies indicating that musicians demonstrate a more complex hierarchical mental representation for music and superior temporal organizational skills. But does acculturation\textemdash the passive exposure to a particular type of music since birth\textemdash also influence the acquisition of these skills? We compared the music synchronization performance of Tunisian and French subjects with music from these two contrasting musical cultures. Twelve musical excerpts were selected from the two popular music cultures, matched for perceived tempo, complexity, and familiarity, and subjects were asked to tap in time with the music. Tapping mode (rate and hierarchical level) varied with subjects' familiarity with the musical idiom, as evidenced by an interaction between musical culture and type of music: participants synchronized at higher hierarchical levels (and over a wider range) with music from their own culture than with an unfamiliar type of music. Thus, passive acculturation as well as explicit music tuition influence our perception and cognition of music.},
  Shorttitle = {Synchronizing with {{Music}}},
  Annotation = {\_eprint: https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1196/annals.1284.053},
  File = {/Users/courtneyhilton/Zotero/storage/T8ZHCNU9/Drake and Heni - 2003 - Synchronizing with Music Intercultural Difference.pdf;/Users/courtneyhilton/Zotero/storage/RER52F2T/annals.1284.html},
  Doi = {10.1196/annals.1284.053}
}


@Article{Filippi2017,
  Author = {Filippi, Piera and Congdon, Jenna V. and Hoang, John and Bowling, Daniel L. and Reber, Stephan A. and Pa{\v s}ukonis, Andrius and Hoeschele, Marisa and Ocklenburg, Sebastian and de Boer, Bart and Sturdy, Christopher B. and Newen, Albert and G{\"u}nt{\"u}rk{\"u}n, Onur},
  Journal = {Proceedings of the Royal Society B: Biological Sciences},
  Month = {},
  Number = {1859},
  Title = {Humans Recognize Emotional Arousal in Vocalizations across All Classes of Terrestrial Vertebrates: {{Evidence}} for Acoustic Universals},
  Volume = {284},
  Year = {2017},
  Issn = {0962-8452, 1471-2954},
  Langid = {english},
  Abstract = {Writing over a century ago, Darwin hypothesized that vocal expression of emotion dates back to our earliest terrestrial ancestors. If this hypothesis is true, we should expect to find cross-species acoustic universals in emotional vocalizations. Studies suggest that acoustic attributes of aroused vocalizations are shared across many mammalian species, and that humans can use these attributes to infer emotional content. But do these acoustic attributes extend to non-mammalian vertebrates? In this study, we asked human participants to judge the emotional content of vocalizations of nine vertebrate species representing three different biological classes\textemdash Amphibia, Reptilia (non-aves and aves) and Mammalia. We found that humans are able to identify higher levels of arousal in vocalizations across all species. This result was consistent across different language groups (English, German and Mandarin native speakers), suggesting that this ability is biologically rooted in humans. Our findings indicate that humans use multiple acoustic parameters to infer relative arousal in vocalizations for each species, but mainly rely on fundamental frequency and spectral centre of gravity to identify higher arousal vocalizations across species. These results suggest that fundamental mechanisms of vocal emotional expression are shared among vertebrates and could represent a homologous signalling system.},
  Copyright = {\textcopyright{} 2017 The Author(s). http://royalsocietypublishing.org/licencePublished by the Royal Society. All rights reserved.},
  Shorttitle = {Humans Recognize Emotional Arousal in Vocalizations across All Classes of Terrestrial Vertebrates},
  Doi = {10.1098/osf.io/rspb.2017.0990}
}


@Article{Fitch1997,
  Author = {Fitch, W Tecumseh},
  Journal = {The Journal of the Acoustical Society of America},
  Pages = {11},
  Title = {Vocal Tract Length and Formant Frequency Dispersion Correlate with Body Size in Rhesus Macaques},
  Year = {1997},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/5DVDJ6JE/Fitch - Vocal tract length and formant frequency dispersio.pdf}
}


@Article{Fitch2002,
  Author = {Fitch, W. Tecumseh and Neubauer, J{\"u}rgen and Herzel, Hanspeter},
  Journal = {Animal Behaviour},
  Month = {},
  Number = {3},
  Pages = {407--418},
  Title = {Calls out of Chaos: {{The}} Adaptive Significance of Nonlinear Phenomena in Mammalian Vocal Production},
  Volume = {63},
  Year = {2002},
  Issn = {0003-3472},
  Keywords = {to read},
  Abstract = {Recent work on human vocal production demonstrates that certain irregular phenomena seen in human pathological voices and baby crying result from nonlinearities in the vocal production system. Equivalent phenomena are quite common in nonhuman mammal vocal repertoires. In particular, bifurcations and chaos are ubiquitous aspects of the normal adult repertoire in many primate species. Here we argue that these phenomena result from properties inherent in the peripheral production mechanism, which allows individuals to generate highly complex and unpredictable vocalizations without requiring equivalently complex neural control mechanisms. We provide examples from the vocal repertoire of rhesus macaques, Macaca mulatta, and other species illustrating the different classes of nonlinear phenomena, and review the concepts from nonlinear dynamics that explicate these calls. Finally, we discuss the evolutionary significance of nonlinear vocal phenomena. We suggest that nonlinear phenomena may subserve individual recognition and the estimation of size or fluctuating asymmetry from vocalizations. Furthermore, neurally `cheap' unpredictability may serve the valuable adaptive function of making chaotic calls difficult to predict and ignore. While noting that nonlinear phenomena are in some cases probably nonadaptive by-products of the physics of the sound-generating mechanism, we suggest that these functional hypotheses provide at least a partial explanation for the ubiquity of nonlinear calls in nonhuman vocal repertoires.},
  Shorttitle = {Calls out of Chaos},
  Doi = {10.1006/anbe.2001.1912}
}


@Misc{Friedman2016,
  Author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  Title = {Lasso and Elastic-Net Regularized Generalized Linear Models. {{Rpackage}} Version 2.0-5.},
  Year = {2016}
}


@Article{Germine2012,
  Author = {Germine, Laura and Nakayama, Ken and Duchaine, Bradley C. and Chabris, Christopher F. and Chatterjee, Garga and Wilmer, Jeremy B.},
  Journal = {Psychonomic Bulletin \& Review},
  Month = {},
  Number = {5},
  Pages = {847--857},
  Title = {Is the {{Web}} as Good as the Lab? {{Comparable}} Performance from {{Web}} and Lab in Cognitive/Perceptual Experiments},
  Volume = {19},
  Year = {2012},
  Issn = {1069-9384, 1531-5320},
  Langid = {english},
  Shorttitle = {Is the {{Web}} as Good as the Lab?},
  File = {/Users/courtneyhilton/Zotero/storage/CNMQ5M2C/Germine et al. - 2012 - Is the Web as good as the lab Comparable performa.pdf},
  Doi = {10.3758/s13423-012-0296-9}
}


@Article{Granier-Deferre2011,
  Author = {{Granier-Deferre}, Carolyn and Bassereau, Sophie and Ribeiro, Aur{\'e}lie and Jacquet, Anne-Yvonne and DeCasper, Anthony J.},
  Journal = {PLoS ONE},
  Month = {},
  Number = {2},
  Title = {A Melodic Contour Repeatedly Experienced by Human Near-Term Fetuses Elicits a Profound Cardiac Reaction One Month after Birth},
  Volume = {6},
  Year = {2011},
  Keywords = {added,memory for music,music [infants]},
  Abstract = {Background   Human hearing develops progressively during the last trimester of gestation. Near-term fetuses can discriminate acoustic features, such as frequencies and spectra, and process complex auditory streams. Fetal and neonatal studies show that they can remember frequently recurring sounds. However, existing data can only show retention intervals up to several days after birth.       Methodology/Principal Findings   Here we show that auditory memories can last at least six weeks. Experimental fetuses were given precisely controlled exposure to a descending piano melody twice daily during the 35th, 36th, and 37th weeks of gestation. Six weeks later we assessed the cardiac responses of 25 exposed infants and 25 naive control infants, while in quiet sleep, to the descending melody and to an ascending control piano melody. The melodies had precisely inverse contours, but similar spectra, identical duration, tempo and rhythm, thus, almost identical amplitude envelopes. All infants displayed a significant heart rate change. In exposed infants, the descending melody evoked a cardiac deceleration that was twice larger than the decelerations elicited by the ascending melody and by both melodies in control infants.       Conclusions/Significance   Thus, 3-weeks of prenatal exposure to a specific melodic contour affects infants `auditory processing' or perception, i.e., impacts the autonomic nervous system at least six weeks later, when infants are 1-month old. Our results extend the retention interval over which a prenatally acquired memory of a specific sound stream can be observed from 3\textendash 4 days to six weeks. The long-term memory for the descending melody is interpreted in terms of enduring neurophysiological tuning and its significance for the developmental psychobiology of attention and perception, including early speech perception, is discussed.},
  File = {/Users/courtneyhilton/Zotero/storage/CEVEAKU9/Granier-Deferre et al. - 2011 - A Melodic Contour Repeatedly Experienced by Human .pdf},
  Doi = {10.1371/journal.pone.0017304}
}


@Article{Hale1990,
  Author = {Hale, Sandra},
  Journal = {Child Development},
  Number = {3},
  Pages = {653--663},
  Title = {A {{Global Developmental Trend}} in {{Cognitive Processing Speed}}},
  Volume = {61},
  Year = {1990},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/K4LNWZ7M/Hale - 2021 - A Global Developmental Trend in Cognitive Processi.pdf}
}


@Article{Hannon2005,
  Author = {Hannon, Erin E. and Trehub, Sandra E.},
  Journal = {Psychological Science},
  Number = {1},
  Pages = {48--55},
  Title = {Metrical Categories in Infancy and Adulthood},
  Volume = {16},
  Year = {2005}
}


@Article{Hartshorne2019,
  Author = {Hartshorne, Joshua K. and {de Leeuw}, Joshua and Goodman, Noah and Jennings, Mariela and O'Donnell, Timothy J.},
  Journal = {Behavior Research Methods},
  Pages = {1782--1803},
  Title = {A Thousand Studies for the Price of One: {{Accelerating}} Psychological Science with {{Pushkin}}},
  Volume = {51},
  Year = {2019},
  Issn = {1554-3528},
  Abstract = {Half of the world's population has internet access. In principle, researchers are no longer limited to subjects they can recruit into the laboratory. Any study that can be run on a computer or mobile device can be run with nearly any demographic anywhere in the world, and in large numbers. This has allowed scientists to effectively run hundreds of experiments at once. Despite their transformative power, such studies remain rare for practical reasons: the need for sophisticated software, the difficulty of recruiting so many subjects, and a lack of research paradigms that make effective use of their large amounts of data, due to such realities as that they require sophisticated software in order to run effectively. We present Pushkin: an open-source platform for designing and conducting massive experiments over the internet. Pushkin allows for a wide range of behavioral paradigms, through integration with the intuitive and flexible jsPsych experiment engine. It also addresses the basic technical challenges associated with massive, worldwide studies, including auto-scaling, extensibility, machine-assisted experimental design, multisession studies, and data security.},
  Doi = {10.3758/s13428-018-1155-z},
  Pmid = {30746644}
}


@Article{Herrmann2007,
  Author = {Herrmann, Esther and Call, Josep and {Hern{\`a}ndez-Lloreda}, Mar{\'i}a Victoria and Hare, Brian and Tomasello, Michael},
  Journal = {Science},
  Month = {},
  Number = {5843},
  Pages = {1360--1366},
  Title = {Humans {{Have Evolved Specialized Skills}} of {{Social Cognition}}: {{The Cultural Intelligence Hypothesis}}},
  Volume = {317},
  Year = {2007},
  Issn = {0036-8075, 1095-9203},
  Langid = {english},
  Shorttitle = {Humans {{Have Evolved Specialized Skills}} of {{Social Cognition}}},
  File = {/Users/courtneyhilton/Zotero/storage/MY77P9DD/Herrmann et al_2007_Humans Have Evolved Specialized Skills of Social Cognition.pdf},
  Doi = {10.1126/science.1146282}
}


@Article{Hilton2020,
  Author = {Hilton, Courtney B. and Asano, Rie and Boeckx, Cedric A.},
  Journal = {Behavioral and Brain Sciences},
  Title = {Why Musical Hierarchies?},
  Year = {2020}
}


@Article{Hilton2021a,
  Author = {Hilton, Courtney B. and Moser, Cody J. and Bertolo, Mila and {Lee-Rubin}, Harry and Bainbridge, Constance M. and Atwood, S. and Simson, Jan and Knox, Dean and Glowacki, Luke and Galbarczyk, Andrzej and Jasienska, Grazyna and Ross, Cody T. and Neff, Mary Beth and Martin, Alia and Cirelli, Laura K. and Trehub, Sandra E. and Song, Jinqi and Kim, Minju and Schachner, Adena and Vardy, Tom A. and Atkinson, Quentin D. and Antfolk, Jan and Madhivanan, Purnima and Siddaiah, Anand and Placek, Caitlyn D. and Salali, Gul Deniz and Keestra, Sarai and Singh, Manvir and Collins, Scott A. and Patton, John Q. and Scaff, Camila and Stieglitz, Jonathan and Moya, Cristina and Sagar, Rohan R. and Wood, Brian M. and Krasnow, Max M. and Mehr, Samuel A.},
  Journal = {bioRxiv},
  Title = {Acoustic Regularities in Infant-Directed Speech and Song across Cultures},
  Year = {2021},
  Langid = {english},
  Abstract = {Humans often produce vocalizations for infants that differ from vocalizations for adults. Is this property common across societies? The forms of infant-directed vocalizations may be shaped by their function in parent-infant communication. If so, infant-directed song and speech should be differentiable from adult-directed song and speech on the basis of their acoustic features, and this property should be relatively invariant across cultures. To test this hypothesis, we built a corpus of 1,614 recordings of infant- and adult-directed singing and speech produced by 411 people living in 21 urban, rural, and small-scale societies. We studied the corpus in a massive online experiment and in a series of acoustic analyses. Na\"ive listeners (N = 13,218) reliably identified infant-directed vocalizations as infant-directed, and adult-directed speech (but not songs) as adult-directed, at rates far higher than chance. Ratings of infant-directed song were the most accurate and the most consistent across all societies; infant-directed speech was accurately identified on average, but inconsistently across societies. To determine the mechanisms underlying these results, we extracted many acoustic features from each recording and identified those that most reliably characterize infant-directed song and speech across cultures, via preregistered exploratory-confirmatory analyses and machine classification. The features distinguishing infant- and adult-directed song and speech concerned pitch, rhythmic, phonetic, and timbral attributes; a hypothesis-free classifier with cross-validation across societies reliably identified all vocalization types, with highest accuracy for infant-directed song. Last, we isolated 12 acoustic features that were predictive of perceived infant-directedness; of these, two pitch attributes (median F0 and its variability) were by far the most explanatory. These findings demonstrate cross-cultural regularities in infant-directed vocalizations that are suggestive of universality; moreover, infant-directed song appears to be more cross-culturally stereotyped than infant-directed speech, informing hypotheses of the functions and evolution of both.},
  Doi = {10.1101/2020.04.09.032995}
}


@Article{Hilton2022,
  Author = {Hilton, Courtney B. and Mehr, Samuel A.},
  Journal = {Behavioral and Brain Sciences},
  Title = {Citizen Science Can Help to Alleviate the Generalizability Crisis},
  Year = {2022}
}


@Article{Holbrook1990,
  Author = {Holbrook, Morris B. and Anand, Punam},
  Journal = {Psychology of Music},
  Month = {},
  Number = {2},
  Pages = {150--162},
  Title = {Effects of {{Tempo}} and {{Situational Arousal}} on the {{Listener}}'s {{Perceptual}} and {{Affective Responses}} to {{Music}}},
  Volume = {18},
  Year = {1990},
  Issn = {0305-7356, 1741-3087},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/7SD8N8YL/Holbrook and Anand - 1990 - Effects of Tempo and Situational Arousal on the Li.pdf},
  Doi = {10.1177/0305735690182004}
}


@Article{Huber2020,
  Author = {Huber, Bernd and Gajos, Krzysztof Z.},
  Editor = {Manning, Victoria},
  Journal = {PLOS ONE},
  Month = {},
  Number = {1},
  Pages = {e0227629},
  Title = {Conducting Online Virtual Environment Experiments with Uncompensated, Unsupervised Samples},
  Volume = {15},
  Year = {2020},
  Issn = {1932-6203},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/68S6CWJD/Huber_Gajos_2020_Conducting online virtual environment experiments with uncompensated,.pdf},
  Doi = {10.1371/journal.pone.0227629}
}


@Article{Husain2002,
  Author = {Husain, Gabriela and Thompson, William Forde and Schellenberg, E. Glenn},
  Journal = {Music Perception},
  Month = {},
  Number = {2},
  Pages = {151--171},
  Title = {Effects of {{Musical Tempo}} and {{Mode}} on {{Arousal}}, {{Mood}}, and {{Spatial Abilities}}},
  Volume = {20},
  Year = {2002},
  Issn = {0730-7829},
  Langid = {english},
  Abstract = {We examined effects of tempo and mode on spatial ability, arousal, and mood. A Mozart sonata was performed by a skilled pianist and recorded as a MIDI file. The file was edited to produce four versions that varied in tempo (fast or slow) and mode (major or minor). Participants listened to a single version and completed measures of spatial ability, arousal, and mood. Performance on the spatial task was superior after listening to music at a fast rather than a slow tempo, and when the music was presented in major rather than minor mode. Tempo manipulations affected arousal but not mood, whereas mode manipulations affected mood but not arousal. Changes in arousal and mood paralleled variation on the spatial task. The findings are consistent with the view that the "Mozart effect" is a consequence of changes in arousal and mood.},
  File = {/Users/courtneyhilton/Zotero/storage/L5STS7YY/Husain et al. - 2002 - Effects of Musical Tempo and Mode on Arousal, Mood.pdf},
  Doi = {10.1525/mp.2002.20.2.151}
}


@Article{Ilie2006,
  Author = {Ilie, Gabriella and Thompson, William Forde},
  Journal = {Music Perception},
  Month = {},
  Number = {4},
  Pages = {319--330},
  Title = {A {{Comparison}} of {{Acoustic Cues}} in {{Music}} and {{Speech}} for {{Three Dimensions}} of {{Affect}}},
  Volume = {23},
  Year = {2006},
  Issn = {0730-7829, 1533-8312},
  Langid = {english},
  Abstract = {Using a three-dimensional model of affect, we compared the affective consequences of manipulating intensity, rate, and pitch height in music and speech. Participants rated 64 music and 64 speech excerpts on valence (pleasant-unpleasant), energy arousal (awake-tired), and tension arousal (tense-relaxed). For music and speech, loud excerpts were judged as more pleasant, energetic, and tense than soft excerpts. Manipulations of rate had overlapping effects on music and speech. Fast music and speech were judged as having greater energy than slow music and speech. However, whereas fast speech was judged as less pleasant than slow speech, fast music was judged as having greater tension than slow music. Pitch height had opposite consequences for music and speech, with high-pitched speech but lowpitched music associated with higher ratings of valence (more pleasant). Interactive effects on judgments were also observed. We discuss similarities and differences between vocal and musical communication of affect, and the need to distinguish between two types of arousal: energy and tension.},
  File = {/Users/courtneyhilton/Zotero/storage/E35GBV34/Ilie and Thompson - 2006 - A Comparison of Acoustic Cues in Music and Speech .pdf},
  Doi = {10.1525/mp.2006.23.4.319}
}


@Article{Imai2008,
  Author = {Imai, Mutsumi and Kita, Sotaro and Nagumo, Miho and Okada, Hiroyuki},
  Journal = {Cognition},
  Month = {},
  Number = {1},
  Pages = {54--65},
  Title = {Sound Symbolism Facilitates Early Verb Learning},
  Volume = {109},
  Year = {2008},
  Issn = {00100277},
  Langid = {english},
  Abstract = {Some words are sound-symbolic in that they involve a non-arbitrary relationship between sound and meaning. Here, we report that 25-month-old children are sensitive to cross-linguistically valid sound-symbolic matches in the domain of action and that this sound symbolism facilitates verb learning in young children. We constructed a set of novel soundsymbolic verbs whose sounds were judged to match certain actions better than others, as confirmed by adult Japanese- as well as English speakers, and by 2- and 3-year-old Japanese-speaking children. These sound-symbolic verbs, together with other novel nonsound-symbolic verbs, were used in a verb learning task with 3-year-old Japanese children. In line with the previous literature, 3-year-olds could not generalize the meaning of novel non-sound-symbolic verbs on the basis of the sameness of action. However, 3-year-olds could correctly generalize the meaning of novel sound-symbolic verbs. These results suggest that iconic scaffolding by means of sound symbolism plays an important role in early verb learning.},
  File = {/Users/courtneyhilton/Zotero/storage/WKBZ8F32/Imai et al. - 2008 - Sound symbolism facilitates early verb learning.pdf},
  Doi = {10.1016/j.cognition.2008.07.015}
}


@Article{Jacoby2019,
  Author = {Jacoby, Nori and Undurraga, Eduardo A. and McPherson, Malinda J. and Vald{\'e}s, Joaqu{\'i}n and Ossand{\'o}n, Tom{\'a}s and McDermott, Josh H.},
  Journal = {Current Biology},
  Month = {},
  Number = {19},
  Pages = {3229--3243},
  Title = {Universal and Non-Universal Features of Musical Pitch Perception Revealed by Singing},
  Volume = {29},
  Year = {2019},
  Issn = {09609822},
  Langid = {english},
  Abstract = {Musical pitch perception is argued to result from nonmusical biological constraints and thus to have similar characteristics across cultures, but its universality remains unclear. We probed pitch representations in residents of the Bolivian Amazon\textemdash the Tsimane', who live in relative isolation from Western culture\textemdash as well as US musicians and non-musicians. Participants sang back tone sequences presented in different frequency ranges. Sung responses of Amazonian and US participants approximately replicated heard intervals on a logarithmic scale, even for tones outside the singing range. Moreover, Amazonian and US reproductions both deteriorated for high-frequency tones even though they were fully audible. But whereas US participants tended to reproduce notes an integer number of octaves above or below the heard tones, Amazonians did not, ignoring the note ``chroma'' (C, D, etc.). Chroma matching in US participants was more pronounced in US musicians than non-musicians, was not affected by feedback, and was correlated with similarity-based measures of octave equivalence as well as the ability to match the absolute f0 of a stimulus in the singing range. The results suggest the cross-cultural presence of logarithmic scales for pitch, and biological constraints on the limits of pitch, but indicate that octave equivalence may be culturally contingent, plausibly dependent on pitch representations that develop from experience with particular musical systems.},
  Doi = {10.1016/j.cub.2019.08.020}
}


@Article{Janata2012,
  Author = {Janata, Petr and Tomic, Stefan T. and Haberman, Jason M.},
  Journal = {Journal of Experimental Psychology: General},
  Number = {1},
  Pages = {54--75},
  Title = {Sensorimotor Coupling in Music and the Psychology of the Groove.},
  Volume = {141},
  Year = {2012},
  Issn = {1939-2222, 0096-3445},
  Langid = {english},
  Abstract = {The urge to move in response to music, combined with the positive affect associated with the coupling of sensory and motor processes while engaging with music (referred to as sensorimotor coupling) in a seemingly effortless way, is commonly described as the feeling of being in the groove. Here, we systematically explore this compelling phenomenon in a population of young adults. We utilize multiple levels of analysis, comprising phenomenological, behavioral, and computational techniques. Specifically, we show (a) that the concept of the groove is widely appreciated and understood in terms of a pleasurable drive toward action, (b) that a broad range of musical excerpts can be appraised reliably for the degree of perceived groove, (c) that the degree of experienced groove is inversely related to experienced difficulty of bimanual sensorimotor coupling under tapping regimes with varying levels of expressive constraint, (d) that high-groove stimuli elicit spontaneous rhythmic movements, and (e) that quantifiable measures of the quality of sensorimotor coupling predict the degree of experienced groove. Our results complement traditional discourse regarding the groove, which has tended to take the psychological phenomenon for granted and has focused instead on the musical and especially the rhythmic qualities of particular genres of music that lead to the perception of groove. We conclude that groove can be treated as a psychological construct and model system that allows for experimental exploration of the relationship between sensorimotor coupling with music and emotion.},
  File = {/Users/courtneyhilton/Zotero/storage/E9CI5HMS/Janata et al. - 2012 - Sensorimotor coupling in music and the psychology .pdf},
  Doi = {10.1037/a0024208}
}


@Article{Kamiloglu2020,
  Author = {Kamilo{\u g}lu, Roza G. and Slocombe, Katie E. and Haun, Daniel BM and Sauter, Disa A.},
  Journal = {Proceedings of the Royal Society B},
  Number = {1929},
  Pages = {20201148},
  Publisher = {{The Royal Society}},
  Title = {Human Listeners' Perception of Behavioural Context and Core Affect Dimensions in Chimpanzee Vocalizations},
  Volume = {287},
  Year = {2020},
  File = {/Users/courtneyhilton/Zotero/storage/TUNHP78Z/KamiloÄŸlu et al. - 2020 - Human listenersâ€™ perception of behavioural context.pdf;/Users/courtneyhilton/Zotero/storage/HGTQCZMA/rspb.2020.html}
}


@Article{Kinzler2007,
  Author = {Kinzler, Katherine D. and Dupoux, Emmanuel and Spelke, Elizabeth S.},
  Journal = {Proceedings of the National Academy of Sciences},
  Number = {30},
  Pages = {12577--12580},
  Title = {The Native Language of Social Cognition},
  Volume = {104},
  Year = {2007},
  Keywords = {infants [not music]}
}


@Article{Kiselev2009,
  Author = {Kiselev, Sergey and Espy, Kimberly Andrews and Sheffield, Tiffany},
  Journal = {Journal of Experimental Child Psychology},
  Month = {},
  Number = {2},
  Pages = {150--166},
  Title = {Age-Related Differences in Reaction Time Task Performance in Young Children},
  Volume = {102},
  Year = {2009},
  Issn = {00220965},
  Langid = {english},
  Abstract = {Performance of reaction time (RT) tasks was investigated in young children and adults to test the hypothesis that age-related differences in processing speed supersede a ``global'' mechanism and are a function of specific differences in task demands and processing requirements. The sample consisted of 54 4-year-olds, 53 5-year-olds, 59 6-year-olds, and 35 adults from Russia. Using the regression approach pioneered by Brinley and the transformation method proposed by Madden and colleagues and Ridderinkhoff and van der Molen, age-related differences in processing speed differed among RT tasks with varying demands. In particular, RTs differed between children and adults on tasks that required response suppression, discrimination of color or spatial orientation, reversal of contingencies of previously learned stimulus\textendash response rules, and greater stimulus\textendash response complexity. Relative costs of these RT task differences were larger than predicted by the global difference hypothesis except for response suppression. Among young children, age-related differences larger than predicted by the global difference hypothesis were evident when tasks required color or spatial orientation discrimination and stimulus\textendash response rule complexity, but not for response suppression or reversal of stimulus\textendash response contingencies. Process-specific, age-related differences in processing speed that support heterochronicity of brain development during childhood were revealed.},
  File = {/Users/courtneyhilton/Zotero/storage/NF2KG3XQ/Kiselev et al. - 2009 - Age-related differences in reaction time task perf.pdf},
  Doi = {10.1016/j.jecp.2008.02.002}
}


@Article{Krumhansl2010a,
  Author = {Krumhansl, Carol L.},
  Journal = {Music Perception},
  Month = {},
  Number = {5},
  Pages = {337--354},
  Title = {Plink: "{{Thin Slices}}" of {{Music}}},
  Volume = {27},
  Year = {2010},
  Issn = {0730-7829, 1533-8312},
  Langid = {english},
  Abstract = {SHORT CLIPS (300 AND 400 MS), TAKEN FROM POPULAR songs from the 1960's through the 2000's,were presented to participants in two experiments to study the detail and contents of musical memory. For 400 ms clips, participants identified both artist and title on more than 25\% of the trials.Very accurate confidence ratings showed that this knowledge was recalled consciously. Performance was somewhat higher if the clip contained a word or partword from the title. Even when a clip was not identified, it conveyed information about emotional content, style and, to some extent, decade of release. Performance on song identification was markedly lower for 300 ms clips, although participants still gave consistent emotion and style judgments, and fairly accurate judgments of decade of release. The decade of release had no effect on identification, emotion consistency, or style consistency. However, older songs were preferred, suggesting that the availability of recorded music alters the pattern of preferences previously assumed to be established during adolescence and early adulthood. Taken together, the results point to extraordinary abilities to identify music based on highly reduced information.},
  Shorttitle = {Plink},
  File = {/Users/courtneyhilton/Zotero/storage/H59QWF65/Krumhansl - 2010 - Plink Thin Slices of Music.pdf},
  Doi = {10.1525/mp.2010.27.5.337}
}


@Article{Levitin1996,
  Author = {Levitin, Daniel J. and Cook, Perry R.},
  Journal = {Perception \& Psychophysics},
  Month = {},
  Number = {6},
  Pages = {927--935},
  Title = {Memory for Musical Tempo: {{Additional}} Evidence That Auditory Memory Is Absolute},
  Volume = {58},
  Year = {1996},
  Issn = {0031-5117, 1532-5962},
  Keywords = {added},
  Langid = {english},
  Abstract = {We report evidence that long-term memory retains absolute (accurate) features of perceptual events. Specifically, we show that memory for music seems to preserve the absolute tempo of the musical performance. In Experiment 1, 46 subjects sang two different popular songs from memory, and their tempos were compared with recorded versions of the songs. Seventy-two percent of the productions on two consecutive trials came within 8\% of the actual tempo, demonstrating accuracy near the perceptual threshold (JND) for tempo. In Experiment 2, a control experiment, we found that folk songs lacking a tempo standard generally have a large variability in tempo; this counters arguments that memory for the tempo of remembered songs is driven by articulatory constraints. The relevance of the present findings to theories of perceptual memory and memory for music is discussed.},
  Shorttitle = {Memory for Musical Tempo},
  Doi = {10.3758/BF03205494}
}


@Article{Liberman2017,
  Author = {Liberman, Zoe and Woodward, Amanda L. and Kinzler, Katherine D.},
  Journal = {Trends in Cognitive Sciences},
  Month = {},
  Number = {7},
  Pages = {556--568},
  Title = {The {{Origins}} of {{Social Categorization}}},
  Volume = {21},
  Year = {2017},
  Issn = {1364-6613, 1879-307X},
  Keywords = {essentialism,infant,intergroup cognition,prejudice,social categorization,stereotype},
  Langid = {english},
  Abstract = {Forming conceptually-rich social categories helps people to navigate the complex social world by allowing them to reason about the likely thoughts, beliefs, actions, and interactions of others, as guided by group membership. Nevertheless, social categorization often has nefarious consequences. We suggest that the foundation of the human ability to form useful social categories is in place in infancy: social categories guide the inferences infants make about the shared characteristics and social relationships of other people. We also suggest that the ability to form abstract social categories may be separable from the eventual negative downstream consequences of social categorization, including prejudice, discrimination, and stereotyping. Although a tendency to form inductively-rich social categories appears early in ontogeny, prejudice based on each particular category dimension may not be inevitable., Social preferences for ingroup members emerge in the first year of life., Preferring to look at or to interact with familiar or similar others does not necessarily indicate an ability to form abstract and inductively-rich social categories., Recent studies using violation of expectation looking-time methods provide clearer evidence that infants can form conceptually-rich social categories., Infants use social group boundaries to guide their inductive generalizations and expectations about social relationships., Social categorization and social preferences are each malleable based on input, experience, and interventions, suggesting that prejudice may not be inevitable.},
  Doi = {10.1016/j.tics.2017.04.004},
  Pmid = {28499741}
}


@Article{Liu2021,
  Author = {Liu, Jingxuan and Hilton, Courtney B. and Bergelson, Elika and Mehr, Samuel A.},
  Journal = {bioRxiv},
  Title = {Language Experience Shapes Music Processing across 40 Tonal, Pitch-Accented, and Non-Tonal Languages},
  Year = {2021},
  Copyright = {All rights reserved},
  Doi = {10.1101/2021.10.18.464888}
}


@Book{Lomax1968,
  Address = {{Washington, DC}},
  Author = {Lomax, Alan},
  Publisher = {{American Association for the Advancement of Science}},
  Title = {Folk Song Style and Culture},
  Year = {1968},
  Keywords = {added},
  Langid = {english}
}


@Article{Lynch1990,
  Author = {Lynch, Michael P. and Eilers, Rebecca E. and Oller, D. Kimbrough and Urbano, Richard C.},
  Journal = {Psychological Science},
  Month = {},
  Number = {4},
  Pages = {272--276},
  Publisher = {{SAGE Publications Inc}},
  Title = {Innateness, {{Experience}}, and {{Music Perception}}},
  Volume = {1},
  Year = {1990},
  Issn = {0956-7976},
  Keywords = {tim-t,timAdded},
  Langid = {english},
  Abstract = {Musical acculturation from infancy to adulthood was studied by testing the abilities of Western 6-month-olds and adults to notice mistunings in melodies based on native Western major, native Western minor, and non-native Javanese pelog scales. Results indicated that infants were similarly able to perceive native and non-native scales. Adults, however, were generally better perceivers of native than non-native scales. These findings suggest that infants are born with an equipotentiality for the perception of scales from a variety of cultures and that subsequent culturally specific experience substantially influences music perception.},
  File = {/Users/courtneyhilton/Zotero/storage/5VW7774F/Lynch et al. - 1990 - Innateness, Experience, and Music Perception.pdf;/Users/courtneyhilton/Zotero/storage/KLAHLNKT/Lynch et al. - 1990 - Innateness, Experience, and Music Perception.pdf},
  Doi = {10.1111/j.1467-9280.1990.tb00213.x}
}


@Article{Lynch1992,
  Author = {Lynch, Michael P. and Eilers, Rebecca E.},
  Journal = {Perception \& Psychophysics},
  Month = {},
  Number = {6},
  Pages = {599--608},
  Title = {A Study of Perceptual Development for Musical Tuning},
  Volume = {52},
  Year = {1992},
  Issn = {1532-5962},
  Keywords = {tim-univ,timAdded},
  Langid = {english},
  Abstract = {Musical tuning perception in infancy and adulthood was explored in three experiments. In Experiment 1, Western adults were tested in detection of randomly located mistunings in a melody based on musical interval patterns from native and nonnative musical scales. Subjects performed better in a Western major scale context than in either a Western augmented or--a Javanese pelog scale context. Because the major scale is used frequently in Western music and, therefore, is more perceptually familiar than either the augmented scale or the pelog scale are, the adults' pattern of performance is suggestive of musical acculturation. Experiments 2 and3 were designed to explore the onset of culturally specific perceptual reorganization for music in the age period that has been found to be important in linguistically specific perceptual reorganization for speech. In Experiment 2, 1-year-olds had a pattern of performance similar to that of the adults, but 6-month-olds could not detect mistunings reliably better than chance. In Experiment 3, another group of 6-month-olds was tested, and a larger degree of mistuning was used so that floor effects might be avoided. These 6-month-olds performed better in the major and augmented scale contexts than in the pelog context, without a reliable performance difference between the major and augmented contexts. Comparison of the results obtained with 6-month-olds and 1-year-olds suggests that culturally specific perceptual reorganization for musical tuning begins to affect perception between these ages, but the 6-month-olds' pattern of results considered alone is not as clear. The 6-month-olds' better performance on the major and augmented interval patterns than on the pelog interval pattern is potentially attributable to either the 6-month.olds' lesser perceptual acculturation than that of the 1-year-olds or perhaps to an innate predisposition for processing of music based on a single fundamental interval, in this case the semitone.},
  File = {/Users/courtneyhilton/Zotero/storage/8JFA6WJH/Lynch and Eilers - 1992 - A study of perceptual development for musical tuni.pdf;/Users/courtneyhilton/Zotero/storage/C9NQH7WS/Lynch and Eilers - 1992 - A study of perceptual development for musical tuni.pdf},
  Doi = {10.3758/BF03211696}
}


@Article{Ma2015,
  Author = {Ma, Weiyi and Thompson, William Forde},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {47},
  Pages = {14563--14568},
  Title = {Human Emotions Track Changes in the Acoustic Environment},
  Volume = {112},
  Year = {2015},
  Issn = {0027-8424, 1091-6490},
  Langid = {english},
  Abstract = {Emotional responses to biologically significant events are essential for human survival. Do human emotions lawfully track changes in the acoustic environment? Here we report that changes in acoustic attributes that are well known to interact with human emotions in speech and music also trigger systematic emotional responses when they occur in environmental sounds, including sounds of human actions, animal calls, machinery, or natural phenomena, such as wind and rain. Three changes in acoustic attributes known to signal emotional states in speech and music were imposed upon 24 environmental sounds. Evaluations of stimuli indicated that human emotions track such changes in environmental sounds just as they do for speech and music. Such changes not only influenced evaluations of the sounds themselves, they also affected the way accompanying facial expressions were interpreted emotionally. The findings illustrate that human emotions are highly attuned to changes in the acoustic environment, and reignite a discussion of Charles Darwin's hypothesis that speech and music originated from a common emotional signal system based on the imitation and modification of environmental sounds.},
  File = {/Users/courtneyhilton/Zotero/storage/BHP7XPR7/Ma and Thompson - 2015 - Human emotions track changes in the acoustic envir.pdf},
  Doi = {10.1073/pnas.1515087112}
}


@Article{Macmillan1985,
  Author = {Macmillan, Neil A and Kaplan, Howard L},
  Journal = {Psychological Bulletin},
  Pages = {15},
  Title = {Detection {{Theory Analysis}} of {{Group Data}}: {{Estimating Sensitivity From Average Hit}} and {{False-Alarm Rates}}},
  Year = {1985},
  Keywords = {d-prime},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/W5GES9SX/Macmillan_Kaplan_1985_Detection Theory Analysis of Group Data.pdf}
}


@Article{Margulis2022,
  Author = {Margulis, Elizabeth H. and Wong, Patrick C. M. and Turnbull, Cara and Kubit, Benjamin M. and McAuley, J. Devin},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {4},
  Pages = {e2110406119},
  Title = {Narratives Imagined in Response to Instrumental Music Reveal Culture-Bounded Intersubjectivity},
  Volume = {119},
  Year = {2022},
  Issn = {0027-8424, 1091-6490},
  Langid = {english},
  Abstract = {The scientific literature sometimes considers music an abstract stimulus, devoid of explicit meaning, and at other times considers it a universal language. Here, individuals in three geographically distinct locations spanning two cultures performed a highly unconstrained task: they provided free-response descriptions of stories they imagined while listening to instrumental music. Tools from natural language processing revealed that listeners provide highly similar stories to the same musical excerpts when they share an underlying culture, but when they do not, the generated stories show limited overlap. These results paint a more complex picture of music's power: music can generate remarkably similar stories in listeners' minds, but the degree to which these imagined narratives are shared depends on the degree to which culture is shared across listeners. Thus, music is neither an abstract stimulus nor a universal language but has semantic affordances shaped by culture, requiring more sustained attention from psychology.},
  File = {/Users/courtneyhilton/Zotero/storage/BHGF9I27/Margulis et al_2022_Narratives imagined in response to instrumental music reveal culture-bounded.pdf},
  Doi = {10.1073/pnas.2110406119}
}


@Article{Martin2012,
  Author = {Martin, Alia and Onishi, Kristine H. and Vouloumanos, Athena},
  Journal = {Cognition},
  Month = {},
  Number = {1},
  Pages = {50--60},
  Title = {Understanding the Abstract Role of Speech in Communication at 12months},
  Volume = {123},
  Year = {2012},
  Issn = {00100277},
  Langid = {english},
  Doi = {10.1016/j.cognition.2011.12.003}
}


@Article{Mehr2014,
  Author = {Mehr, Samuel A.},
  Journal = {Journal of Research in Music Education},
  Number = {1},
  Pages = {78--88},
  Title = {Music in the Home: {{New}} Evidence for an Intergenerational Link},
  Volume = {62},
  Year = {2014},
  Issn = {0022-4294, 1945-0095},
  Keywords = {added},
  Langid = {english},
  Shorttitle = {Music in the {{Home}}},
  Doi = {10.1177/0022429413520008}
}


@Article{Mehr2016,
  Author = {Mehr, Samuel A. and Song, Lee Ann and Spelke, Elizabeth S.},
  Journal = {Psychological Science},
  Number = {4},
  Pages = {486--501},
  Title = {For 5-Month-Old Infants, Melodies Are Social},
  Volume = {27},
  Year = {2016},
  Doi = {10.1177/0956797615626691},
  Pmid = {26917211}
}


@Article{Mehr2017,
  Author = {Mehr, Samuel A and Spelke, Elizabeth S.},
  Journal = {Developmental Science},
  Number = {2},
  Title = {Shared Musical Knowledge in 11-month-old Infants},
  Volume = {21},
  Year = {2017},
  Issn = {1363-755X},
  Pmid = {28229502}
}


@Article{Mehr2017b,
  Author = {Mehr, Samuel A. and Krasnow, Max M.},
  Journal = {Evolution and Human Behavior},
  Number = {5},
  Pages = {674--684},
  Title = {Parent-Offspring Conflict and the Evolution of Infant-Directed Song},
  Volume = {38},
  Year = {2017},
  Doi = {10.1016/j.evolhumbehav.2016.12.005},
  Pmid = {28857689}
}


@Article{Mehr2018a,
  Author = {Mehr, Samuel A. and Singh, Manvir and York, Hunter and Glowacki, Luke and Krasnow, Max M.},
  Journal = {Current Biology},
  Number = {3},
  Pages = {356--368},
  Title = {Form and Function in Human Song},
  Volume = {28},
  Year = {2018},
  Issn = {0960-9822},
  Keywords = {culture,diversity,evolution,form,function,music,song,universality,vocalization},
  Langid = {english},
  Doi = {10.1016/j.cub.2017.12.042},
  Pmid = {29395919},
  Pmcid = {PMC5805477}
}


@Article{Mehr2019,
  Author = {Mehr, Samuel A. and Singh, Manvir and Knox, Dean and Ketter, Daniel M. and {Pickens-Jones}, Daniel and Atwood, S. and Lucas, Christopher and Jacoby, Nori and Egner, Alena A. and Hopkins, Erin J. and Howard, Rhea M. and Hartshorne, Joshua K. and Jennings, Mariela V. and Simson, Jan and Bainbridge, Constance M. and Pinker, Steven and O'Donnell, Timothy J. and Krasnow, Max M. and Glowacki, Luke},
  Journal = {Science},
  Number = {6468},
  Pages = {957--970},
  Title = {Universality and Diversity in Human Song},
  Volume = {366},
  Year = {2019},
  Issn = {0036-8075, 1095-9203},
  Langid = {english},
  Abstract = {Cross-cultural analysis of song It is unclear whether there are universal patterns to music across cultures. Mehr et al. examined ethnographic data and observed music in every society sampled (see the Perspective by Fitch and Popescu). For songs specifically, three dimensions characterize more than 25\% of the performances studied: formality of the performance, arousal level, and religiosity. There is more variation in musical behavior within societies than between societies, and societies show similar levels of within-society variation in musical behavior. At the same time, one-third of societies significantly differ from average for any given dimension, and half of all societies differ from average on at least one dimension, indicating variability across cultures. Science, this issue p. eaax0868; see also p. 944 Structured Abstract INTRODUCTIONMusic is often assumed to be a human universal, emerging from an evolutionary adaptation specific to music and/or a by-product of adaptations for affect, language, motor control, and auditory perception. But universality has never actually been systematically demonstrated, and it is challenged by the vast diversity of music across cultures. Hypotheses of the evolutionary function of music are also untestable without comprehensive and representative data on its forms and behavioral contexts across societies. RATIONALEWe conducted a natural history of song: a systematic analysis of the features of vocal music found worldwide. It consists of a corpus of ethnographic text on musical behavior from a representative sample of mostly small-scale societies, and a discography of audio recordings of the music itself. We then applied tools of computational social science, which minimize the influence of sampling error and other biases, to answer six questions. Does music appear universally? What kinds of behavior are associated with song, and how do they vary among societies? Are the musical features of a song indicative of its behavioral context (e.g., infant care)? Do the melodic and rhythmic patterns of songs vary systematically, like those patterns found in language? And how prevalent is tonality across musical idioms? RESULTSAnalysis of the ethnography corpus shows that music appears in every society observed; that variation in song events is well characterized by three dimensions (formality, arousal, religiosity); that musical behavior varies more within societies than across them on these dimensions; and that music is regularly associated with behavioral contexts such as infant care, healing, dance, and love. Analysis of the discography corpus shows that identifiable acoustic features of songs (accent, tempo, pitch range, etc.) predict their primary behavioral context (love, healing, etc.); that musical forms vary along two dimensions (melodic and rhythmic complexity); that melodic and rhythmic bigrams fall into power-law distributions; and that tonality is widespread, perhaps universal. CONCLUSIONMusic is in fact universal: It exists in every society (both with and without words), varies more within than between societies, regularly supports certain types of behavior, and has acoustic features that are systematically related to the goals and responses of singers and listeners. But music is not a fixed biological response with a single prototypical adaptive function: It is produced worldwide in diverse behavioral contexts that vary in formality, arousal, and religiosity. Music does appear to be tied to specific perceptual, cognitive, and affective faculties, including language (all societies put words to their songs), motor control (people in all societies dance), auditory analysis (all musical systems have signatures of tonality), and aesthetics (their melodies and rhythms are balanced between monotony and chaos). These analyses show how applying the tools of computational social science to rich bodies of humanistic data can reveal both universal features and patterns of variability in culture, addressing long-standing debates about each. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/366/6468/eaax0868/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Studying world music systematically.We used primary ethnographic text and field recordings of song performances to build two richly annotated cross-cultural datasets: NHS Ethnography and NHS Discography. The original material in each dataset was annotated by humans (both amateur and expert) and by automated algorithms. What is universal about music, and what varies? We built a corpus of ethnographic text on musical behavior from a representative sample of the world's societies, as well as a discography of audio recordings. The ethnographic corpus reveals that music (including songs with words) appears in every society observed; that music varies along three dimensions (formality, arousal, religiosity), more within societies than across them; and that music is associated with certain behavioral contexts such as infant care, healing, dance, and love. The discography\textemdash analyzed through machine summaries, amateur and expert listener ratings, and manual transcriptions\textemdash reveals that acoustic features of songs predict their primary behavioral context; that tonality is widespread, perhaps universal; that music varies in rhythmic and melodic complexity; and that elements of melodies and rhythms found worldwide follow power laws. Songs exhibit universal patterns across cultures. Songs exhibit universal patterns across cultures.},
  Copyright = {Copyright \textcopyright{} 2019, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  Doi = {10.1126/science.aax0868}
}


@Article{Mehr2020,
  Author = {Mehr, Samuel A. and Krasnow, Max M. and Bryant, Gregory A. and Hagen, Edward H.},
  Journal = {Behavioral and Brain Sciences},
  Month = {},
  Pages = {1--41},
  Title = {Origins of Music in Credible Signaling},
  Year = {2020},
  Issn = {0140-525X, 1469-1825},
  Langid = {english},
  Abstract = {Abstract             Music comprises a diverse category of cognitive phenomena that likely represent both the effects of psychological adaptations that are specific to music (e.g., rhythmic entrainment) and the effects of adaptations for non-musical functions (e.g., auditory scene analysis). How did music evolve? Here, we show that prevailing views on the evolution of music \textemdash{} that music is a byproduct of other evolved faculties, evolved for social bonding, or evolved to signal mate quality \textemdash{} are incomplete or wrong. We argue instead that music evolved as a credible signal in at least two contexts: coalitional interactions and infant care. Specifically, we propose that (1) the production and reception of coordinated, entrained rhythmic displays is a co-evolved system for credibly signaling coalition strength, size, and coordination ability; and (2) the production and reception of infant-directed song is a co-evolved system for credibly signaling parental attention to secondarily altricial infants. These proposals, supported by interdisciplinary evidence, suggest that basic features of music, such as melody and rhythm, result from adaptations in the proper domain of human music. The adaptations provide a foundation for the cultural evolution of music in its actual domain, yielding the diversity of musical forms and musical behaviors found worldwide.},
  File = {/Users/courtneyhilton/Zotero/storage/XJPJWWFD/Mehr et al. - 2020 - Origins of music in credible signaling.pdf},
  Doi = {10.1017/S0140525X20000345}
}


@Article{Mehr2021,
  Author = {Mehr, Samuel A and Krasnow, Max and Bryant, Gregory A. and Hagen, Edward H},
  Journal = {Behavioral and Brain Sciences},
  Title = {Origins of Music in Credible Signaling},
  Year = {2021},
  Abstract = {How did music evolve? We show that prevailing views on the evolution of music \textemdash{} that music is a byproduct of other evolved faculties, that music evolved for social bonding, and that music evolved to signal mate quality \textemdash{} are incomplete or wrong. We argue instead that music evolved as a credible signal in at least two contexts: coalitional interactions and infant care. We suggest that basic features of music, including melody and rhythm, result from adaptations in the proper domain of human music, providing a foundation that cultural evolution shapes into its actual domain.},
  Doi = {10.31234/osf.io/nrqb3}
}


@Article{Mendoza2021,
  Author = {Mendoza, Jennifer K. and Fausey, Caitlin M.},
  Journal = {Developmental Science},
  Title = {Everyday Music in Infancy},
  Year = {2021},
  Langid = {english},
  Abstract = {Infants enculturate to their soundscape over the first year of life, yet theories of how they do so rarely make contact with details about the sounds available in everyday life. Here, we report on properties of a ubiquitous early ecology in which foundational skills get built: music. We captured daylong recordings from 35 infants ages 6-12 months at home and fully double-coded 467 hours of everyday sounds for music and its features, tunes, and voices. Analyses of this first-of-its-kind corpus revealed two distributional properties of infants' everyday musical ecology. First, infants encountered vocal music in over half, and instrumental in over threequarters, of everyday music. Live sources generated one-third, and recorded sources threequarters, of everyday music. Second, infants did not encounter each individual tune and voice in their day equally often. Instead, the most available identity cumulated to many more seconds of the day than would be expected under a uniform distribution. These properties of everyday music in human infancy are different from what is discoverable in environments highly constrained by context (e.g., laboratories) and time (e.g., minutes rather than hours). Together with recent insights about the everyday motor, language, and visual ecologies of infancy, these findings reinforce an emerging priority to build theories of development that address the opportunities and challenges of real input encountered by real learners.},
  File = {/Users/courtneyhilton/Zotero/storage/ZSDQVQKP/Mendoza and Fausey - 2019 - Everyday Music in Infancy.pdf},
  Doi = {10.31234/osf.io/sqatb}
}


@Article{Morton1977,
  Author = {Morton, Eugene S.},
  Journal = {The American Naturalist},
  Number = {981},
  Pages = {855--869},
  Title = {On the Occurrence and Significance of Motivation-Structural Rules in Some Bird and Mammal Sounds},
  Volume = {111},
  Year = {1977},
  Issn = {0003-0147},
  Abstract = {The convergent use of harsh, low-frequency sounds by hostile animals and more pure tonelike, high frequency sounds by fearful or appeasing animals is discussed in an evolutionary context. It is proposed that many sounds in species' repertoires are evolved from motivation-structural rules derived from selection pressures favoring the use of communication instead of, or in conjunction with, fighting to attain resources. The use of this concept should further the appreciation of the relationship between sound structure and function.}
}


@Book{Nettl2005,
  Address = {{Urbana and Chicago}},
  Author = {Nettl, Bruno},
  Edition = {2nd Edition},
  Publisher = {{University of Illinois Press}},
  Title = {The {{Study}} of {{Ethnomusicology}}},
  Year = {2005}
}


@Article{Norris2014,
  Author = {Norris, Ray P and Harney, Bill Yidumduma},
  Journal = {Journal of Astronomical History and Heritage},
  Number = {2},
  Pages = {15},
  Title = {Songlines and {{Navigation}} in {{Wardaman}} and Other {{Australian Aboriginal Cultures}}},
  Volume = {17},
  Year = {2014},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/KWVMMJ25/Norris and Harney - Songlines and Navigation in Wardaman and other Aus.pdf}
}


@Article{Nuckolls1999,
  Author = {Nuckolls, Janis B.},
  Journal = {Annual Review of Anthropology},
  Month = {},
  Number = {1},
  Pages = {225--252},
  Title = {The {{Case}} for {{Sound Symbolism}}},
  Volume = {28},
  Year = {1999},
  Issn = {0084-6570, 1545-4290},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/5LVQRIHT/Nuckolls - 1999 - The Case for Sound Symbolism.pdf},
  Doi = {10.1146/annurev.anthro.28.1.225}
}


@Article{Owren2001,
  Author = {Owren, Michael J. and Rendall, Drew},
  Journal = {Evolutionary Anthropology},
  Month = {},
  Number = {2},
  Pages = {58--71},
  Title = {Sound on the Rebound: {{Bringing}} Form and Function Back to the Forefront in Understanding Nonhuman Primate Vocal Signaling},
  Volume = {10},
  Year = {2001},
  Issn = {1520-6505},
  Keywords = {acoustic primatology,affect induction,calls,direct acoustic effects,learned affect,meaning},
  Langid = {english},
  Copyright = {Copyright \textcopyright{} 2001 Wiley-Liss, Inc.},
  Shorttitle = {Sound on the Rebound},
  Doi = {10.1002/evan.1014}
}


@Article{Patten2018,
  Author = {Patten, K. Jakob and McBeath, Michael K. and Baxter, Leslie C.},
  Journal = {Auditory Perception \& Cognition},
  Month = {},
  Number = {3-4},
  Pages = {150--172},
  Title = {Harmonicity: {{Behavioral}} and {{Neural Evidence}} for {{Functionality}} in {{Auditory Scene Analysis}}},
  Volume = {1},
  Year = {2018},
  Issn = {2574-2442, 2574-2450},
  Langid = {english},
  Abstract = {This study examines sound perception and tests the functionality of harmonicity as a metric of auditory preference and regional brain activity. Most salient sounds contain harmonic overtones, a reliable acoustic natural regularity, which we model to computationally define harmonicity. In Experiment 1, listeners rated pairwise similarity of 20 intensity- and f0-matched diverse acoustic stimuli. A multidimensional scaling solution revealed two principal timbre dimensions: tone color and harmonicity. These dimensions, respectively, correlated with the computational measures of spectralenvelope slope (r = 0.51) and RMSE deviation from a harmonic series (r = 0.78). The harmonicity dimension also correlated with rated preference (r = 0.48). This enabled creation of an ordered stimuluspreference continuum that ranged from positive (piano) to negative (hawk screech), used for fMRI tests in Experiment 2. Those results revealed a bilateral primary auditory cortical network for all sounds, but with inharmonic exemplars only producing activation in a frontoinsular cortical network, possibly associated with oddball detection. The results are consistent with dissonant sounds violating the regularity of acoustic-object harmonicity, thereby engaging brain regions associated with unexpected stimuli and attentional focus. The findings confirm harmonicity is an important auditory dimension related to preference, with promise to elicit systematic patterns of cortical network activation.},
  Shorttitle = {Harmonicity},
  File = {/Users/courtneyhilton/Zotero/storage/QDU5RZ44/Patten et al. - 2018 - Harmonicity Behavioral and Neural Evidence for Fu.pdf},
  Doi = {10.1080/25742442.2019.1609307}
}


@Article{Perani2010,
  Author = {Perani, Daniela and Saccuman, Maria Cristina and Scifo, Paola and Spada, Danilo and Andreolli, Guido and Rovelli, Rosanna and Baldoli, Cristina and Koelsch, Stefan},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {10},
  Pages = {4758--4763},
  Title = {Functional Specializations for Music Processing in the Human Newborn Brain},
  Volume = {107},
  Year = {2010},
  Issn = {0027-8424, 1091-6490},
  Keywords = {added,evo,neuro},
  Langid = {english},
  Abstract = {In adults, specific neural systems with right-hemispheric weighting are necessary to process pitch, melody, and harmony as well as structure and meaning emerging from musical sequences. It is not known to what extent the specialization of these systems results from long-term exposure to music or from neurobiological constraints. One way to address this question is to examine how these systems function at birth, when auditory experience is minimal. We used functional MRI to measure brain activity in 1- to 3-day-old newborns while they heard excerpts of Western tonal music and altered versions of the same excerpts. Altered versions either included changes of the tonal key or were permanently dissonant. Music evoked predominantly right-hemispheric activations in primary and higher order auditory cortex. During presentation of the altered excerpts, hemodynamic responses were significantly reduced in the rig1ht auditory cortex, and activations emerged in the left inferior frontal cortex and limbic structures. These results demonstrate that the infant brain shows a hemispheric specialization in processing music as early as the first postnatal hours. Results also indicate that the neural architecture underlying music processing in newborns is sensitive to changes in tonal key as well as to differences in consonance and dissonance.},
  Doi = {10.1073/pnas.0909074107},
  Pmid = {20176953}
}


@Article{Peretz1998,
  Author = {Peretz, Isabelle and Gaudreau, Danielle and Bonnel, Anne-Marie},
  Journal = {Memory \& Cognition},
  Month = {},
  Number = {5},
  Pages = {884--902},
  Title = {Exposure Effects on Music Preference and Recognition},
  Volume = {26},
  Year = {1998},
  Issn = {0090-502X, 1532-5946},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/ZSUX5JDM/Peretz et al_1998_Exposure effects on music preference and recognition.pdf},
  Doi = {10.3758/BF03201171}
}


@Article{Perlman2021,
  Author = {Perlman, Marcus and Paul, Jing and Lupyan, Gary},
  Journal = {Journal of Experimental Psychology: General},
  Month = {},
  Title = {Vocal Communication of Magnitude across Language, Age, and Auditory Experience.},
  Year = {2021},
  Issn = {1939-2222, 0096-3445},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/M32KS54Y/Perlman et al. - 2021 - Vocal communication of magnitude across language, .pdf},
  Doi = {10.1037/xge0001103}
}


@Article{Phillips-Silver2005,
  Author = {{Phillips-Silver}, J.},
  Journal = {Science},
  Month = {},
  Number = {5727},
  Pages = {1430--1430},
  Title = {Feeling the {{Beat}}: {{Movement Influences Infant Rhythm Perception}}},
  Volume = {308},
  Year = {2005},
  Issn = {0036-8075, 1095-9203},
  Keywords = {music [infants]},
  Shorttitle = {Feeling the {{Beat}}},
  File = {/Users/courtneyhilton/Zotero/storage/8VQZ4BG4/Phillips-Silver - 2005 - Feeling the Beat Movement Influences Infant Rhyth.pdf},
  Doi = {10.1126/science.1110922}
}


@Article{Powell2013,
  Author = {Powell, L. J. and Spelke, E. S.},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {41},
  Pages = {E3965-E3972},
  Title = {Preverbal Infants Expect Members of Social Groups to Act Alike},
  Volume = {110},
  Year = {2013},
  Issn = {0027-8424, 1091-6490},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/GYFEU77R/Powell and Spelke - 2013 - Preverbal infants expect members of social groups .pdf},
  Doi = {10.1073/pnas.1304326110}
}


@Book{Reinecke2015,
  Author = {Reinecke, K. and Gajos, K. Z.},
  Title = {Proceedings of the 18th {{ACM Conference}} on {{Computer Supported Cooperative Work}} \& {{Social Computing}}.},
  Year = {2015},
  Isbn = {978-1-4503-2922-4},
  Langid = {english},
  Annotation = {OCLC: 994255523}
}


@Article{Rohrmeier2012,
  Author = {Rohrmeier, Martin and Rebuschat, Patrick},
  Journal = {Topics in Cognitive Science},
  Month = {},
  Number = {4},
  Pages = {525--553},
  Title = {Implicit {{Learning}} and {{Acquisition}} of {{Music}}},
  Volume = {4},
  Year = {2012},
  Issn = {17568757},
  Langid = {english},
  Doi = {10.1111/j.1756-8765.2012.01223.x}
}


@Article{Saffran2000,
  Author = {Saffran, Jenny R and Loman, Michelle M and Robertson, Rachel R.W},
  Journal = {Cognition},
  Month = {},
  Number = {1},
  Pages = {B15-B23},
  Title = {Infant Memory for Musical Experiences},
  Volume = {77},
  Year = {2000},
  Issn = {0010-0277},
  Keywords = {added,memory for music,music [infants]},
  Abstract = {Recent findings suggest that infants can remember words from stories over 2 week delays (Jusczyk, P. W., \&amp; Hohne, E. A. (1997). Infants' memory for spoken words. Science, 277, 1984\textendash 1986). Because music, like language, presents infants with a massively complex auditory learning task, it is possible that infant memory for musical stimuli is equally powerful. Seven-month-old infants heard two Mozart sonata movements daily for 2 weeks. Following a 2 week retention interval, the infants were tested on passages of the familiarized music, and passages taken from similar but novel music. Results from two experiments suggest that the infants retained the familiarized music in long-term memory, and that their listening preferences were affected by the extent to which familiar passages were removed from the musical contexts within which they were originally learned.},
  Doi = {10.1016/S0010-0277(00)00095-0}
}


@Article{Schellenberg2008b,
  Author = {Schellenberg, E. Glenn and Peretz, Isabelle and Vieillard, Sandrine},
  Journal = {Cognition \& Emotion},
  Month = {},
  Number = {2},
  Pages = {218--237},
  Title = {Liking for Happy- and Sad-Sounding Music: {{Effects}} of Exposure},
  Volume = {22},
  Year = {2008},
  Issn = {0269-9931, 1464-0600},
  Langid = {english},
  Shorttitle = {Liking for Happy- and Sad-Sounding Music},
  File = {/Users/courtneyhilton/Zotero/storage/KVW2I9V7/Schellenberg et al_2008_Liking for happy- and sad-sounding music.pdf},
  Doi = {10.1080/02699930701350753}
}


@Article{Scherer2001,
  Author = {Scherer, Klaus R. and Banse, Rainer and Wallbott, Harald G.},
  Journal = {Journal of Cross-Cultural Psychology},
  Month = {},
  Number = {1},
  Pages = {76--92},
  Title = {Emotion {{Inferences}} from {{Vocal Expression Correlate Across Languages}} and {{Cultures}}},
  Volume = {32},
  Year = {2001},
  Issn = {0022-0221, 1552-5422},
  Langid = {english},
  Abstract = {Whereas the perception of emotion from facial expression has been extensively studied cross-culturally, little is known about judges' ability to infer emotion from vocal cues. This article reports the results from a study conducted in nine countries in Europe, the United States, and Asia on vocal emotion portrayals of anger, sadness, fear, joy, and neutral voice as produced by professional German actors. Data show an overall accuracy of 66\% across all emotions and countries. Although accuracy was substantially better than chance, there were sizable differences ranging from 74\% in Germany to 52\% in Indonesia. However, patterns of confusion were very similar across all countries. These data suggest the existence of similar inference rules from vocal expression across cultures. Generally, accuracy decreased with increasing language dissimilarity from German in spite of the use of language-free speech samples. It is concluded that culture- and language-specific paralinguistic patterns may influence the decoding process.},
  File = {/Users/courtneyhilton/Zotero/storage/BBAJDQ7S/Scherer et al. - 2001 - Emotion Inferences from Vocal Expression Correlate.pdf},
  Doi = {10.1177/0022022101032001009}
}


@Article{Schubert2004,
  Author = {Schubert, Emery},
  Journal = {Music Perception},
  Month = {},
  Number = {4},
  Pages = {561--585},
  Title = {Modeling {{Perceived Emotion With Continuous Musical Features}}},
  Volume = {21},
  Year = {2004},
  Issn = {0730-7829, 1533-8312},
  Langid = {english},
  Abstract = {The relationship between musical features and perceived emotion was investigated by using continuous response methodology and time-series analysis. Sixty-seven participants responded to four pieces of Romantic music expressing different emotions. Responses were sampled once per second on a two-dimensional emotion space (happy-sad valence and aroused-sleepy). Musical feature variables of loudness, tempo, melodic contour, texture, and spectral centroid (related to perceived timbral sharpness) were coded. Musical feature variables were differenced and used as predictors in two univariate linear regression models of valence and arousal for each of the four pieces. Further adjustments were made to the models to correct for serial correlation. The models explained from 33\% to 73\% of variation in univariate perceived emotion. Changes in loudness and tempo were associated positively with changes in arousal, but loudness was dominant. Melodic contour varied positively with valence, though this finding was not conclusive. Texture and spectral centroid did not produce consistent predictions. This methodology facilitates a more ecologically valid investigation of emotion in music and, importantly in the present study, enabled the approximate identification of the lag between musical features and perceived emotion. Responses were made 1 to 3 s after a change in the causal musical event, with sudden changes in loudness producing response lags from zero (nearly instantaneous) to 1 s. Other findings, interactions, and ramifications of the methodology are also discussed.},
  File = {/Users/courtneyhilton/Zotero/storage/MVXEYHQZ/Schubert - 2004 - Modeling Perceived Emotion With Continuous Musical.pdf},
  Doi = {10.1525/mp.2004.21.4.561}
}


@Article{Sheskin2020,
  Author = {Sheskin, Mark and Scott, Kimberly and Mills, Candice M. and Bergelson, Elika and Bonawitz, Elizabeth and Spelke, Elizabeth S. and {Fei-Fei}, Li and Keil, Frank C. and Gweon, Hyowon and Tenenbaum, Joshua B. and {Jara-Ettinger}, Julian and Adolph, Karen E. and Rhodes, Marjorie and Frank, Michael C. and Mehr, Samuel A. and Schulz, Laura},
  Journal = {Trends in Cognitive Sciences},
  Month = {},
  Publisher = {{Elsevier}},
  Title = {Online Developmental Science to Foster Innovation, Access, and Impact},
  Year = {2020},
  Issn = {1364-6613, 1879-307X},
  Keywords = {best practices,cognitive development,collaboration,internet,research methods,social development},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/QY45U3KC/Sheskin et al. - 2020 - Online Developmental Science to Foster Innovation,.pdf;/Users/courtneyhilton/Zotero/storage/B7XMJPYR/S1364-6613(20)30145-5.html},
  Doi = {10.1016/j.tics.2020.06.004},
  Pmid = {32624386}
}


@Article{Sievers2013,
  Author = {Sievers, Beau and Polansky, Larry and Casey, Michael and Wheatley, Thalia},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {1},
  Pages = {70--75},
  Title = {Music and Movement Share a Dynamic Structure That Supports Universal Expressions of Emotion},
  Volume = {110},
  Year = {2013},
  Issn = {0027-8424, 1091-6490},
  Keywords = {added,cross-cultural,cross-modal},
  Langid = {english},
  Abstract = {Music moves us. Its kinetic power is the foundation of human behaviors as diverse as dance, romance, lullabies, and the military march. Despite its significance, the music-movement relationship is poorly understood. We present an empirical method for testing whether music and movement share a common structure that affords equivalent and universal emotional expressions. Our method uses a computer program that can generate matching examples of music and movement from a single set of features: rate, jitter (regularity of rate), direction, step size, and dissonance/visual spikiness. We applied our method in two experiments, one in the United States and another in an isolated tribal village in Cambodia. These experiments revealed three things: (i) each emotion was represented by a unique combination of features, (ii) each combination expressed the same emotion in both music and movement, and (iii) this common structure between music and movement was evident within and across cultures.},
  Doi = {10.1073/pnas.1209023110},
  Pmid = {23248314}
}


@Article{Sievers2019,
  Author = {Sievers, Beau and Lee, Caitlyn and Haslett, William and Wheatley, Thalia},
  Journal = {Proceedings of the Royal Society B},
  Number = {1906},
  Title = {A Multi-Sensory Code for Emotional Arousal},
  Volume = {286},
  Year = {2019},
  Langid = {english},
  Abstract = {People express emotion using their voice, face and movement, as well as through abstract forms as in art, architecture and music. The structure of these expressions often seems intuitively linked to its meaning: romantic poetry is written in flowery curlicues, while the logos of death metal bands use spiky script. Here, we show that these associations are universally understood because they are signalled using a multi-sensory code for emotional arousal. Specifically, variation in the central tendency of the frequency spectrum of a stimulus\textemdash its spectral centroid\textemdash is used by signal senders to express emotional arousal, and by signal receivers to make emotional arousal judgements. We show that this code is used across sounds, shapes, speech and human body movements, providing a strong multi-sensory signal that can be used to efficiently estimate an agent's level of emotional arousal.}
}


@Article{Singh2018,
  Author = {Singh, Manvir},
  Journal = {Behavioral and Brain Sciences},
  Pages = {1--62},
  Title = {The Cultural Evolution of Shamanism},
  Volume = {41},
  Year = {2018},
  Issn = {0140-525X, 1469-1825},
  Langid = {english},
  Pmid = {28679454}
}


@Article{Tomasello2005,
  Author = {Tomasello, Michael and Carpenter, Malinda and Call, Josep and Behne, Tanya and Moll, Henrike},
  Journal = {Behavioral and Brain Sciences},
  Month = {},
  Number = {05},
  Pages = {675--691},
  Title = {Understanding and Sharing Intentions: {{The}} Origins of Cultural Cognition},
  Volume = {28},
  Year = {2005},
  Issn = {1469-1825},
  Abstract = {We propose that the crucial difference between human cognition and that of other species is the ability to participate with others in collaborative activities with shared goals and intentions: shared intentionality. Participation in such activities requires not only especially powerful forms of intention reading and cultural learning, but also a unique motivation to share psychological states with others and unique forms of cognitive representation for doing so. The result of participating in these activities is species-unique forms of cultural cognition and evolution, enabling everything from the creation and use of linguistic symbols to the construction of social norms and individual beliefs to the establishment of social institutions. In support of this proposal we argue and present evidence that great apes (and some children with autism) understand the basics of intentional action, but they still do not participate in activities involving joint intentions and attention (shared intentionality). Human children's skills of shared intentionality develop gradually during the first 14 months of life as two ontogenetic pathways intertwine: (1) the general ape line of understanding others as animate, goal-directed, and intentional agents; and (2) a species-unique motivation to share emotions, experience, and activities with other persons. The developmental outcome is children's ability to construct dialogic cognitive representations, which enable them to participate in earnest in the collectivity that is human cognition.},
  Shorttitle = {Understanding and Sharing Intentions},
  Doi = {10.1017/S0140525X05000129}
}


@Article{Trehub1993,
  Author = {Trehub, Sandra E. and Unyk, Anna M. and Trainor, Laurel J.},
  Journal = {Infant Behavior and Development},
  Number = {2},
  Pages = {193--211},
  Title = {Adults Identify Infant-Directed Music across Cultures},
  Volume = {16},
  Year = {1993},
  Keywords = {added,ids_cross-cultural,music [infants]},
  Doi = {10.1016/0163-6383(93)80017-3}
}


@Article{Trehub1993a,
  Author = {Trehub, Sandra E. and Unyk, Anna M. and Trainor, Laurel J.},
  Journal = {Infant Behavior and Development},
  Number = {3},
  Pages = {285--295},
  Title = {Maternal Singing in Cross-Cultural Perspective},
  Volume = {16},
  Year = {1993},
  Keywords = {added,music [infants]}
}


@Article{Trehub1998a,
  Author = {Trehub, Sandra E. and Trainor, Laurel},
  Journal = {Advances in Infancy Research},
  Pages = {43--78},
  Title = {Singing to Infants: {{Lullabies}} and Play Songs},
  Volume = {12},
  Year = {1998},
  Keywords = {nhs},
  Shorttitle = {Singing to Infants}
}


@Article{Trehub2015a,
  Author = {Trehub, Sandra E. and Ghazban, Niusha and Corbeil, Mari{\`e}ve},
  Journal = {Annals of the New York Academy of Sciences},
  Month = {},
  Number = {1},
  Pages = {186--192},
  Title = {Musical Affect Regulation in Infancy},
  Volume = {1337},
  Year = {2015},
  Issn = {00778923},
  Keywords = {added},
  Langid = {english},
  Shorttitle = {Musical Affect Regulation in Infancy},
  Doi = {10.1111/nyas.12622}
}


@Article{Vouloumanos2014,
  Author = {Vouloumanos, Athena and Martin, Alia and Onishi, Kristine H.},
  Journal = {Developmental Science},
  Number = {6},
  Pages = {872--879},
  Title = {Do 6-Month-Olds Understand That Speech Can Communicate?},
  Volume = {17},
  Year = {2014},
  Issn = {1467-7687},
  Langid = {english},
  Abstract = {Adults and 12-month-old infants recognize that even unfamiliar speech can communicate information between third parties, suggesting that they can separate the communicative function of speech from its lexical content. But do infants recognize that speech can communicate due to their experience understanding and producing language, or do they appreciate that speech is communicative earlier, with little such experience? We examined whether 6-month-olds recognize that speech can communicate information about an object. Infants watched a Communicator selectively grasp one of two objects (target). During test, the Communicator could no longer reach the objects; she turned to a Recipient and produced speech (a nonsense word) or non-speech (coughing). Infants looked longer when the Recipient selected the non-target than the target object when the Communicator spoke but not when she coughed \textendash{} unless the Recipient had previously witnessed the Communicator's selective grasping of the target object. Our results suggest that at 6 months, with a receptive vocabulary of no more than a handful of commonly used words, infants possess some abstract understanding of the communicative function of speech. This understanding may provide an early mechanism for language and knowledge acquisition.},
  Copyright = {\textcopyright{} 2014 John Wiley \& Sons Ltd},
  File = {/Users/courtneyhilton/Zotero/storage/2LE2KJ2E/Vouloumanos et al. - 2014 - Do 6-month-olds understand that speech can communi.pdf;/Users/courtneyhilton/Zotero/storage/6SCTKKVJ/desc.html},
  Doi = {10.1111/desc.12170}
}


@Article{Weninger2013,
  Author = {Weninger, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W. and Mortillaro, Marcello and Scherer, Klaus R.},
  Journal = {Frontiers in Psychology},
  Title = {On the {{Acoustics}} of {{Emotion}} in {{Audio}}: {{What Speech}}, {{Music}}, and {{Sound}} Have in {{Common}}},
  Volume = {4},
  Year = {2013},
  Issn = {1664-1078},
  Langid = {english},
  Abstract = {Without doubt, there is emotional information in almost any kind of sound received by humans every day: be it the affective state of a person transmitted by means of speech; the emotion intended by a composer while writing a musical piece, or conveyed by a musician while performing it; or the affective state connected to an acoustic event occurring in the environment, in the soundtrack of a movie, or in a radio play. In the field of affective computing, there is currently some loosely connected research concerning either of these phenomena, but a holistic computational model of affect in sound is still lacking. In turn, for tomorrow's pervasive technical systems, including affective companions and robots, it is expected to be highly beneficial to understand the affective dimensions of ``the sound that something makes,'' in order to evaluate the system's auditory environment and its own audio output. This article aims at a first step toward a holistic computational model: starting from standard acoustic feature extraction schemes in the domains of speech, music, and sound analysis, we interpret the worth of individual features across these three domains, considering four audio databases with observer annotations in the arousal and valence dimensions. In the results, we find that by selection of appropriate descriptors, cross-domain arousal, and valence regression is feasible achieving significant correlations with the observer annotations of up to 0.78 for arousal (training on sound and testing on enacted speech) and 0.60 for valence (training on enacted speech and testing on music). The high degree of cross-domain consistency in encoding the two main dimensions of affect may be attributable to the co-evolution of speech and music from multimodal affect bursts, including the integration of nature sounds for expressive effects.},
  Shorttitle = {On the {{Acoustics}} of {{Emotion}} in {{Audio}}},
  File = {/Users/courtneyhilton/Zotero/storage/VGAR4TX2/Weninger et al. - 2013 - On the Acoustics of Emotion in Audio What Speech,.pdf},
  Doi = {10.3389/fpsyg.2013.00292}
}


@Article{Winkler2009,
  Author = {Winkler, Istv{\'a}n and H{\'a}den, G{\'a}bor P. and Ladinig, Olivia and Sziller, Istv{\'a}n and Honing, Henkjan},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {7},
  Pages = {2468--2471},
  Title = {Newborn Infants Detect the Beat in Music},
  Volume = {106},
  Year = {2009},
  Issn = {0027-8424, 1091-6490},
  Keywords = {added,evo,music [infants]},
  Langid = {english},
  Abstract = {To shed light on how humans can learn to understand music, we need to discover what the perceptual capabilities with which infants are born. Beat induction, the detection of a regular pulse in an auditory signal, is considered a fundamental human trait that, arguably, played a decisive role in the origin of music. Theorists are divided on the issue whether this ability is innate or learned. We show that newborn infants develop expectation for the onset of rhythmic cycles (the downbeat), even when it is not marked by stress or other distinguishing spectral features. Omitting the downbeat elicits brain activity associated with violating sensory expectations. Thus, our results strongly support the view that beat perception is innate.},
  File = {/Users/courtneyhilton/Zotero/storage/HN53CUCR/Winkler et al. - 2009 - Newborn Infants Detect the Beat in Music.pdf},
  Doi = {10.1073/pnas.0809035106},
  Pmid = {19171894}
}


@Article{Witek2014,
  Author = {Witek, Maria A. G. and Clarke, Eric F. and Wallentin, Mikkel and Kringelbach, Morten L. and Vuust, Peter},
  Editor = {{Canal-Bruland}, Rouwen},
  Journal = {PLoS ONE},
  Month = {},
  Number = {4},
  Title = {Syncopation, {{Body-Movement}} and {{Pleasure}} in {{Groove Music}}},
  Volume = {9},
  Year = {2014},
  Issn = {1932-6203},
  Langid = {english},
  Abstract = {Moving to music is an essential human pleasure particularly related to musical groove. Structurally, music associated with groove is often characterised by rhythmic complexity in the form of syncopation, frequently observed in musical styles such as funk, hip-hop and electronic dance music. Structural complexity has been related to positive affect in music more broadly, but the function of syncopation in eliciting pleasure and body-movement in groove is unknown. Here we report results from a web-based survey which investigated the relationship between syncopation and ratings of wanting to move and experienced pleasure. Participants heard funk drum-breaks with varying degrees of syncopation and audio entropy, and rated the extent to which the drum-breaks made them want to move and how much pleasure they experienced. While entropy was found to be a poor predictor of wanting to move and pleasure, the results showed that medium degrees of syncopation elicited the most desire to move and the most pleasure, particularly for participants who enjoy dancing to music. Hence, there is an inverted U-shaped relationship between syncopation, body-movement and pleasure, and syncopation seems to be an important structural factor in embodied and affective responses to groove.},
  File = {/Users/courtneyhilton/Zotero/storage/8SNFL9DB/Witek et al. - 2014 - Syncopation, Body-Movement and Pleasure in Groove .PDF},
  Doi = {10.1371/journal.pone.0094446}
}


@Article{Xiao2017,
  Author = {Xiao, Naiqi G. and Quinn, Paul C. and Liu, Shaoying and Ge, Liezhong and Pascalis, Olivier and Lee, Kang},
  Journal = {Developmental Science},
  Month = {},
  Title = {Older but Not Younger Infants Associate Own-Race Faces with Happy Music and Other-Race Faces with Sad Music},
  Year = {2017},
  Issn = {1467-7687},
  Langid = {english},
  Abstract = {We used a novel intermodal association task to examine whether infants associate own- and other-race faces with music of different emotional valences. Three- to 9-month-olds saw a series of neutral own- or other-race faces paired with happy or sad musical excerpts. Three- to 6-month-olds did not show any specific association between face race and music. At 9~months, however, infants looked longer at own-race faces paired with happy music than at own-race faces paired with sad music. Nine-month-olds also looked longer at other-race faces paired with sad music than at other-race faces paired with happy music. These results indicate that infants with nearly exclusive own-race face experience develop associations between face race and music emotional valence in the first year of life. The potential implications of such associations for developing racial biases in early childhood are discussed.},
  Doi = {10.1111/desc.12537}
}


@Article{Yamamoto2007,
  Author = {Yamamoto, Masahira and Naga, Shinobu and Shimizu, Jun},
  Journal = {Psychology of Music},
  Month = {},
  Number = {2},
  Pages = {249--275},
  Title = {Positive Musical Effects on Two Types of Negative Stressful Conditions},
  Volume = {35},
  Year = {2007},
  Issn = {0305-7356, 1741-3087},
  Langid = {english},
  File = {/Users/courtneyhilton/Zotero/storage/FPPIE63R/Yamamoto et al. - 2007 - Positive musical effects on two types of negative .pdf},
  Doi = {10.1177/0305735607070375}
}


@Article{Yan2021,
  Author = {Yan, Ran and Jessani, Ghazal and Spelke, Elizabeth and de Villiers, Peter and de Villiers, Jill and Mehr, Samuel},
  Journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  Title = {Across Demographics and Recent History, Most Parents Sing to Their Infants and Toddlers Daily},
  Year = {2021},
  Keywords = {Developmental Psychology,home environment,infants,music,parenting,Social and Behavioral Sciences,toddlers},
  Abstract = {Music is universally prevalent in human society and is a salient component of the lives of young families. Here, we studied the frequency of singing and playing recorded music in the home using surveys of parents with infants (N = 945). We found that most parents sing to their infant on a daily basis, and the frequency of infant-directed singing is unrelated to parents' income or ethnicity. Two reliable individual differences emerged, however: (1) fathers sing less than mothers, and (2) as infants grow older, parents sing less. Moreover, the laer effect of child age was specific to singing and was not reflected in reports of the frequency of playing recorded music. Last, we meta-analyzed reports of the frequency of infant-directed singing and found lile change in its frequency over the last 30 years, despite substantial changes in the technological environment in the home. These findings, consistent with theories of the psychological functions of music, in general, and infant-directed singing, in particular, demonstrate the everyday nature of music in infancy.},
  File = {/Users/courtneyhilton/Zotero/storage/DEFUVBGD/Yan et al. - 2021 - Across demographics and recent history, most paren.pdf},
  Doi = {10.31234/osf.io/fy5bh}
}


@Article{Yu2021,
  Author = {Yu, Christine S. -P. and McBeath, Michael K. and Glenberg, Arthur M.},
  Journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  Month = {},
  Title = {The Gleam-Glum Effect: /I:/ Versus /{$\lambda$}/ Phonemes Generically Carry Emotional Valence.},
  Year = {2021},
  Issn = {1939-1285, 0278-7393},
  Langid = {english},
  Abstract = {The gleam-glum effect is a novel sound symbolic finding that words with the /i:/-phoneme (like gleam) are perceived more positive emotionally than matched words with the /K/-phoneme (like glum). We provide data that not only confirm the effect but also are consistent with an explanation that /i:/ and /K/ articulation tend to co-occur with activation of positive versus negative emotional facial musculature respectively. Three studies eliminate selection bias by including all applicable English words from the English Lexicon Project (Balota et al., 2007) and the Warriner et al. (2013) database and every possible Mandarin Pinyin combination that differ only in the middle phoneme (/i:/ vs /K/). In Study 1, 61 U.S. undergraduates rated monosyllabic English /i:/ words as robustly more positive than matched /K/ words. Study 2 analyzed the Warriner et al. (2013) valence ratings, extending the gleam-glum effect to all applicable words in the database. In Study 3, 38 U.S. participants (using English) and 37 participants in China (using Mandarin Pinyin) rated word pairs under three conditions that moderate musculature activity: Read aloud (Enhance), read silently (Control), and read silently while chewing gum (Interfere). Indeed, the effect was both replicated and was significantly larger when facial musculature was enhanced than when interfered with, and the two language populations did not significantly differ. These findings confirm a robust gleam-glum effect, despite semantic noise, in English and Mandarin Pinyin. Furthermore, these data are consistent with the hypothesis that this type of sound symbolism arises from the overlap in muscles used both in articulation and emotion expression.},
  Shorttitle = {The Gleam-Glum Effect},
  File = {/Users/courtneyhilton/Zotero/storage/AA56FCNT/Yu et al. - 2021 - The gleam-glum effect i versus Î» phonemes ge.pdf},
  Doi = {10.1037/xlm0001017}
}


@Article{Zentner2010,
  Author = {Zentner, Marcel and Eerola, Tuomas},
  Journal = {Proceedings of the National Academy of Sciences},
  Month = {},
  Number = {13},
  Pages = {5768--5773},
  Title = {Rhythmic Engagement with Music in Infancy},
  Volume = {107},
  Year = {2010},
  Issn = {0027-8424, 1091-6490},
  Keywords = {added,evo,music [infants]},
  Langid = {english},
  Abstract = {Humans have a unique ability to coordinate their motor movements to an external auditory stimulus, as in music-induced foot tapping or dancing. This behavior currently engages the attention of scholars across a number of disciplines. However, very little is known about its earliest manifestations. The aim of the current research was to examine whether preverbal infants engage in rhythmic behavior to music. To this end, we carried out two experiments in which we tested 120 infants (aged 5\textendash 24 months). Infants were exposed to various excerpts of musical and rhythmic stimuli, including isochronous drumbeats. Control stimuli consisted of adult- and infant-directed speech. Infants' rhythmic movements were assessed by multiple methods involving manual coding from video excerpts and innovative 3D motion-capture technology. The results show that (i) infants engage in significantly more rhythmic movement to music and other rhythmically regular sounds than to speech; (ii) infants exhibit tempo flexibility to some extent (e.g., faster auditory tempo is associated with faster movement tempo); and (iii) the degree of rhythmic coordination with music is positively related to displays of positive affect. The findings are suggestive of a predisposition for rhythmic movement in response to music and other metrically regular sounds.},
  Doi = {10.1073/pnas.1000121107},
  Pmid = {20231438}
}
